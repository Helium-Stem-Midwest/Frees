{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted value of Y: 13.26\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "X1 = np.array([3, 4, 4, 7])\n",
    "X2 = np.array([10, 9, 7, 5])\n",
    "y = np.array([1.1, 5.4, 14.7, 25.7])\n",
    "\n",
    "# Design matrix\n",
    "X = np.column_stack((np.ones_like(X1), X1, X2))\n",
    "\n",
    "# Ridge regression coefficients\n",
    "lam = 2.121\n",
    "beta_hat = np.linalg.inv(X.T @ X + lam * np.eye(3)) @ X.T @ y\n",
    "\n",
    "# New data point\n",
    "new_X1 = 5\n",
    "new_X2 = 8\n",
    "\n",
    "# Predicted value\n",
    "y_hat = beta_hat[0] + beta_hat[1] * new_X1 + beta_hat[2] * new_X2\n",
    "\n",
    "print(f\"Predicted value of Y: {y_hat:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bound of 95% prediction interval: 66.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Given data\n",
    "intercept = 14.37632\n",
    "slope = 0.75461\n",
    "residual_se = 6.993\n",
    "n = 30\n",
    "complaints_mean = 1998 / 30\n",
    "\n",
    "# New data point\n",
    "new_complaints = 50\n",
    "\n",
    "# Predicted value\n",
    "y_hat = intercept + slope * new_complaints\n",
    "\n",
    "# Calculate the upper bound of the 95% prediction interval\n",
    "t_critical = t.ppf(0.975, df=n-2)\n",
    "denominator = np.sum([(x - complaints_mean)**2 for x in range(1, n+1)])\n",
    "se_hat = residual_se * np.sqrt(1 + 1/n + (new_complaints - complaints_mean)**2 / denominator)\n",
    "upper_bound = y_hat + t_critical * se_hat\n",
    "\n",
    "print(f\"Upper bound of 95% prediction interval: {upper_bound:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upper bound of 90% prediction interval: 13.74\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Given data\n",
    "y = np.array([10, 8, 9, 5, 12, 4, 8, 8, 11, 5, 7, 3])\n",
    "\n",
    "# Forecast for y₁₆\n",
    "y_hat = 8\n",
    "\n",
    "# Calculate sample standard deviation\n",
    "y_mean = np.mean(y)\n",
    "s = np.sqrt(np.sum((y - y_mean)**2) / (len(y) - 1))\n",
    "\n",
    "# Calculate upper bound of 90% prediction interval\n",
    "alpha = 0.05  # For a 90% prediction interval\n",
    "z_alpha2 = norm.ppf(1 - alpha/2)\n",
    "upper_bound = y_hat + z_alpha2 * s * np.sqrt(1 + 1/len(y))\n",
    "\n",
    "print(f\"Upper bound of 90% prediction interval: {upper_bound:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Given data\n",
    "y = np.array([10, 8, 9, 5, 12, 4, 8, 8, 11, 5, 7, 3])\n",
    "\n",
    "# Forecast for y₁₆\n",
    "y_hat = 8\n",
    "\n",
    "# Calculate sample standard deviation\n",
    "y_mean = np.mean(y)\n",
    "s = np.sqrt(np.sum((y - y_mean)**2) / (len(y) - 1))\n",
    "\n",
    "# Calculate upper bound of 90% prediction interval\n",
    "alpha = 0.05  # For a 90% prediction interval\n",
    "z_alpha2 = norm.ppf(1 - alpha/2)\n",
    "upper_bound = y_hat + z_alpha2 * s * np.sqrt(1 + 1/len(y))\n",
    "\n",
    "print(f\"Upper bound of 90% prediction interval: {upper_bound:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate Ratio for Low Risk: 0.16488615712986132\n",
      "Rate Ratio for High Risk: 2.212670152438822\n",
      "Predicted Mean Claim Count for Low Risk: 0.30999472431080527\n",
      "Predicted Mean Claim Count for High Risk: 4.159937291496249\n",
      "Percentage Change for Low Risk: 83.51138428701387\n",
      "Percentage Change for High Risk: 121.26701524388221\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Define the coefficients\n",
    "intercept = 0.6313\n",
    "low = -1.8025\n",
    "high = 0.7942\n",
    "\n",
    "# Exponentiate the coefficients to get rate ratios\n",
    "low_ratio = np.exp(low)\n",
    "high_ratio = np.exp(high)\n",
    "\n",
    "# Print the rate ratios\n",
    "print(\"Rate Ratio for Low Risk:\", low_ratio)\n",
    "print(\"Rate Ratio for High Risk:\", high_ratio)\n",
    "\n",
    "# Calculate the predicted mean claim counts\n",
    "medium_mean = np.exp(intercept)\n",
    "low_mean = medium_mean * low_ratio\n",
    "high_mean = medium_mean * high_ratio\n",
    "\n",
    "# Print the predicted mean claim counts\n",
    "print(\"Predicted Mean Claim Count for Low Risk:\", low_mean)\n",
    "print(\"Predicted Mean Claim Count for High Risk:\", high_mean)\n",
    "\n",
    "# Calculate the percentage changes\n",
    "percentage_change_low = (1 - low_mean / medium_mean) * 100\n",
    "percentage_change_high = (high_mean / medium_mean - 1) * 100\n",
    "\n",
    "# Print the percentage changes\n",
    "print(\"Percentage Change for Low Risk:\", percentage_change_low)\n",
    "print(\"Percentage Change for High Risk:\", percentage_change_high)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson residual: 1.9266\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from math import exp\n",
    "\n",
    "# Parameter estimates\n",
    "intercept = -0.6327\n",
    "w_coef = 0.0307\n",
    "wt_coef = 0.4522\n",
    "c2_coef = -0.2045\n",
    "c3_coef = -0.4422\n",
    "c4_coef = -0.4424\n",
    "\n",
    "# Given observation\n",
    "w = 28.3\n",
    "wt = 3.05\n",
    "c = 2\n",
    "y = 8\n",
    "\n",
    "# Calculate the predicted mean count (μ)\n",
    "log_mu = intercept + w_coef * w + wt_coef * wt\n",
    "if c == 2:\n",
    "    log_mu += c2_coef\n",
    "elif c == 3:\n",
    "    log_mu += c3_coef\n",
    "elif c == 4:\n",
    "    log_mu += c4_coef\n",
    "\n",
    "mu = exp(log_mu)\n",
    "\n",
    "# Calculate the Pearson residual\n",
    "pearson_residual = (y - mu) / np.sqrt(mu)\n",
    "\n",
    "print(f\"Pearson residual: {pearson_residual:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.5244005127080409, 0.27559948729195916, 0.6085721482493255)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "# Initial probability and corresponding z-value (probit value)\n",
    "initial_p = 0.3\n",
    "z_initial = norm.ppf(initial_p)\n",
    "\n",
    "# Coefficient for x2 and change in x2\n",
    "beta_x2 = 0.4\n",
    "change_in_x2 = 2\n",
    "delta_z = beta_x2 * change_in_x2\n",
    "\n",
    "# New z-value\n",
    "z_new = z_initial + delta_z\n",
    "\n",
    "# New probability\n",
    "new_p = norm.cdf(z_new)\n",
    "z_initial, z_new, new_p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([(('x2', 'x4'), 3.1622776601683795),\n",
       "  (('x1', 'x3'), 5.0990195135927845),\n",
       "  (('x1', 'x2'), 6.082762530298219),\n",
       "  (('x1', 'x4'), 7.280109889280518),\n",
       "  (('x3', 'x4'), 8.54400374531753),\n",
       "  (('x2', 'x3'), 9.219544457292887)],\n",
       " ('x2', 'x4'))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the observations\n",
    "observations = {\n",
    "    'x1': np.array([2, 5]),\n",
    "    'x2': np.array([8, 6]),\n",
    "    'x3': np.array([1, 0]),\n",
    "    'x4': np.array([9, 3])\n",
    "}\n",
    "\n",
    "# Calculate the Euclidean distance between all pairs of observations\n",
    "def euclidean_distance(p1, p2):\n",
    "    return np.sqrt(np.sum((p1 - p2) ** 2))\n",
    "\n",
    "# Initialize a dictionary to hold the distances\n",
    "distances = {}\n",
    "\n",
    "# Calculate the distances between each pair of points\n",
    "for i, (point_i_key, point_i_value) in enumerate(observations.items()):\n",
    "    for point_j_key, point_j_value in list(observations.items())[i+1:]:\n",
    "        # Calculate the distance between the points\n",
    "        distances[(point_i_key, point_j_key)] = euclidean_distance(point_i_value, point_j_value)\n",
    "\n",
    "distances_sorted = sorted(distances.items(), key=lambda item: item[1])\n",
    "\n",
    "# Identify the closest pairs for both Andrew's complete linkage and Allison's single linkage approaches\n",
    "# Andrew's complete linkage approach will look for the maximum intra-cluster distance after merging, which isn't the focus here\n",
    "# Allison's single linkage approach will look for the minimum distance between any member of two clusters, which is our current distance calculation\n",
    "closest_pair_single_linkage = distances_sorted[0][0]  # The pair with the smallest distance\n",
    "\n",
    "# Return the sorted distances and the closest pair for single linkage\n",
    "distances_sorted, closest_pair_single_linkage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.666666666666664"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the observations\n",
    "observations = np.array([13, 40, 60, 71])\n",
    "\n",
    "# Calculate distances between all pairs of observations\n",
    "distances = []\n",
    "for i in range(len(observations)):\n",
    "    for j in range(i+1, len(observations)):\n",
    "        distances.append(abs(observations[i] - observations[j]))\n",
    "\n",
    "# Calculate height of final fuse using complete linkage (maximum distance)\n",
    "h_complete = max(distances)\n",
    "\n",
    "# Calculate height of final fuse using average linkage (average distance)\n",
    "h_average = np.mean(distances)\n",
    "\n",
    "# Calculate |h_complete - h_average|\n",
    "difference = abs(h_complete - h_average)\n",
    "\n",
    "difference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 209.25\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "y = [100, 98, 117, 99, 111]\n",
    "s_3 = 105.0\n",
    "m_1 = 99.0\n",
    "n = len(y)\n",
    "\n",
    "# Calculate Simple Moving Average at t=5\n",
    "s_5 = sum(y[1:]) / 4  # Updated index for the correct window size\n",
    "\n",
    "# Calculate Exponential Smoothed Estimate at t=5\n",
    "w = 2 / (n + 1)\n",
    "m_5 = w * y[4] + (1 - w) * m_1\n",
    "\n",
    "# Calculate the sum of simple moving average and exponential smoothed estimate\n",
    "result = s_5 + m_5\n",
    "print(\"Result:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33002.60466988728,\n",
       " 34252.64074074074,\n",
       " 31563.02820512821,\n",
       " 28229.30558955529,\n",
       " 28443.764019851118)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate RSS for each region and each model based on the formula provided\n",
    "\n",
    "def rss(sum_y_squared, sum_y, n):\n",
    "    return sum_y_squared - (sum_y ** 2) / n\n",
    "\n",
    "# Model I\n",
    "rss_model_I = rss(24327, 881, 54) + rss(90848, 1766, 46)\n",
    "\n",
    "# Model II\n",
    "rss_model_II = rss(4026, 256, 27) + rss(95337, 2093, 65) + rss(15812, 298, 8)\n",
    "\n",
    "# Model III\n",
    "rss_model_III = rss(745, 75, 14) + rss(58952, 1628, 65) + rss(55478, 944, 21)\n",
    "\n",
    "# Model IV\n",
    "rss_model_IV = rss(9227, 495, 43) + rss(105948, 2152, 57)\n",
    "\n",
    "# Model V\n",
    "rss_model_V = rss(8912, 462, 39) + rss(31799, 861, 30) + rss(74464, 1324, 31)\n",
    "\n",
    "rss_model_I, rss_model_II, rss_model_III, rss_model_IV, rss_model_V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.09838031634704124, 0.4028956728543884, 0.3999425924653071)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Observed and predicted values\n",
    "y = np.array([6.0, 7.0, 4.0, 8.0, 9.0])\n",
    "mu_hat = np.array([5.8, 8.3, 4.2, 6.9, 8.8])\n",
    "\n",
    "# Calculation of deviance residual for the third observation\n",
    "d_3 = np.sign(y[2] - mu_hat[2]) * np.sqrt(2 * (y[2] * np.log(y[2] / mu_hat[2]) - (y[2] - mu_hat[2])))\n",
    "\n",
    "# Calculate total scaled deviance\n",
    "D = 2 * np.sum(y * np.log(y / mu_hat) - (y - mu_hat))\n",
    "\n",
    "# Calculate Pearson Chi-square statistic\n",
    "X2 = np.sum((y - mu_hat) ** 2 / mu_hat)\n",
    "\n",
    "d_3, D, X2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7774283295028426, 4.492934782608694)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given values\n",
    "TSS = 8.267\n",
    "SSR = 1.840\n",
    "\n",
    "# Calculate R^2 for the regression of Verbal on Quantitative\n",
    "R_squared = 1 - (SSR / TSS)\n",
    "\n",
    "# Calculate VIF for Verbal Score\n",
    "VIF = 1 / (1 - R_squared)\n",
    "R_squared, VIF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2624"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the grid of probabilities P(Blue)\n",
    "p_blue = np.array([\n",
    "    [0.81, 0.09, 0.62, 0.99, 0.40],\n",
    "    [0.41, 0.70, 0.04, 0.30, 0.32],\n",
    "    [0.74, 0.24, 0.49, 0.85, 0.41],\n",
    "    [0.62, 0.29, 0.17, 0.81, 0.50],\n",
    "    [0.03, 0.77, 0.54, 0.85, 0.17]\n",
    "])\n",
    "\n",
    "# Calculate 1 - P(Blue)\n",
    "p_not_blue = 1 - p_blue\n",
    "\n",
    "# Calculate the minimum error for each cell (Bayes error for each cell)\n",
    "bayes_errors = np.minimum(p_blue, p_not_blue)\n",
    "\n",
    "# Calculate the average Bayes error rate (since each combination is equally likely)\n",
    "bayes_error_rate = np.mean(bayes_errors)\n",
    "bayes_error_rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5.511059907834103, 3.4000000000000004, 2.111059907834103)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Time series data\n",
    "t = np.arange(1, 11)  # Time periods\n",
    "y = np.array([13, 16, 18, 21, 21, 22, 25, 28, 31, 28])  # Observations\n",
    "\n",
    "# Model parameters\n",
    "beta_0 = 12.2\n",
    "beta_1 = 1.8\n",
    "\n",
    "# Model predictions\n",
    "y_pred = beta_0 + beta_1 * t\n",
    "\n",
    "# Split into development and validation subsets\n",
    "y_dev = y[:6]  # First six observations\n",
    "y_val = y[6:]  # Last four observations\n",
    "t_val = t[6:]  # Time for validation\n",
    "y_pred_val = y_pred[6:]  # Predictions for validation\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = np.mean(np.abs((y_val - y_pred_val) / y_val)) * 100\n",
    "\n",
    "# Calculate Mean Square Error (MSE)\n",
    "mse = np.mean((y_val - y_pred_val) ** 2)\n",
    "\n",
    "# Absolute difference between MAPE and MSE\n",
    "abs_diff = np.abs(mape - mse)\n",
    "mape, mse, abs_diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.893300321766805, 0.7830494381854455)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Function to calculate entropy\n",
    "def entropy(p):\n",
    "    if p == 0 or p == 1:\n",
    "        return 0\n",
    "    q = 1 - p\n",
    "    return -p * math.log2(p) - q * math.log2(q)\n",
    "\n",
    "# Gina's split by gender\n",
    "total_male = 215\n",
    "volleyball_male = 164\n",
    "non_volleyball_male = 51\n",
    "p_male = volleyball_male / total_male\n",
    "entropy_male = entropy(p_male)\n",
    "\n",
    "total_female = 285\n",
    "volleyball_female = 171\n",
    "non_volleyball_female = 114\n",
    "p_female = volleyball_female / total_female\n",
    "entropy_female = entropy(p_female)\n",
    "\n",
    "# Charles's split by class\n",
    "total_A = 194\n",
    "volleyball_A = 144\n",
    "non_volleyball_A = 50\n",
    "p_A = volleyball_A / total_A\n",
    "entropy_A = entropy(p_A)\n",
    "\n",
    "total_B = 125\n",
    "volleyball_B = 114\n",
    "non_volleyball_B = 11\n",
    "p_B = volleyball_B / total_B\n",
    "entropy_B = entropy(p_B)\n",
    "\n",
    "total_C = 181\n",
    "volleyball_C = 77\n",
    "non_volleyball_C = 104\n",
    "p_C = volleyball_C / total_C\n",
    "entropy_C = entropy(p_C)\n",
    "\n",
    "# Weighted entropy calculation\n",
    "total_entropy_gina = (total_male * entropy_male + total_female * entropy_female) / (total_male + total_female)\n",
    "total_entropy_charles = (total_A * entropy_A + total_B * entropy_B + total_C * entropy_C) / (total_A + total_B + total_C)\n",
    "\n",
    "total_entropy_gina, total_entropy_charles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.23606797749979,\n",
       " 3.1622776601683795,\n",
       " 6.324555320336759,\n",
       " 5.0,\n",
       " 5.0,\n",
       " 7.0710678118654755)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "# Define the points\n",
    "x1 = (2, 5)\n",
    "x2 = (4, 6)\n",
    "x3 = (1, 2)\n",
    "x4 = (8, 3)\n",
    "\n",
    "# Function to calculate Euclidean distance\n",
    "def euclidean_distance(p1, p2):\n",
    "    return sqrt((p2[0] - p1[0])**2 + (p2[1] - p1[1])**2)\n",
    "\n",
    "# Calculate distances between each pair\n",
    "d12 = euclidean_distance(x1, x2)\n",
    "d13 = euclidean_distance(x1, x3)\n",
    "d14 = euclidean_distance(x1, x4)\n",
    "d23 = euclidean_distance(x2, x3)\n",
    "d24 = euclidean_distance(x2, x4)\n",
    "d34 = euclidean_distance(x3, x4)\n",
    "\n",
    "d12, d13, d14, d23, d24, d34\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.1622776601683795, 6.324555320336759)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqrt(10), sqrt(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-20.076999999999998"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given actual y values\n",
    "y = np.array([19, 32, 17, 13, 6, 15])\n",
    "\n",
    "# The 5th row of matrix H (1-indexed in mathematical notation, so it's the 4th index in Python)\n",
    "H_5 = np.array([0.184, 0.411, 0.443, 0.146])\n",
    "\n",
    "# Calculate the predicted value for the 5th observation (dot product of H_5 with y)\n",
    "y_hat_5 = np.dot(H_5, y[:4])  # Only using the first four elements as we have only four values in H_5\n",
    "\n",
    "# Calculate the residual for the 5th observation\n",
    "e_5 = y[4] - y_hat_5\n",
    "e_5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63.86"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for Model II (result of ridge regression)\n",
    "beta_0 = 10.03\n",
    "beta_1 = 1.73\n",
    "beta_2 = 2.51\n",
    "beta_3 = 3.03\n",
    "\n",
    "# Predictor values\n",
    "X_1 = 9\n",
    "X_2 = 8\n",
    "X_3 = 6\n",
    "\n",
    "# Calculate the predicted value of Y using Model II's coefficients\n",
    "predicted_Y = beta_0 + (beta_1 * X_1) + (beta_2 * X_2) + (beta_3 * X_3)\n",
    "predicted_Y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.896139238860381"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Predicted values for expected loss per customer\n",
    "loss_male_Q = 148\n",
    "loss_male_R = 545\n",
    "loss_female_Q = 446\n",
    "loss_female_R = 4024\n",
    "\n",
    "# Convert the predicted values to the log scale\n",
    "log_loss_male_Q = np.log(loss_male_Q)\n",
    "log_loss_male_R = np.log(loss_male_R)\n",
    "log_loss_female_Q = np.log(loss_female_Q)\n",
    "log_loss_female_R = np.log(loss_female_R)\n",
    "\n",
    "# Calculate the beta for the interaction of Territory R and Female\n",
    "# log_loss_female_R = beta_0 + beta_Female + beta_R + beta_Female:R\n",
    "# log_loss_female_Q = beta_0 + beta_Female\n",
    "# log_loss_male_R = beta_0 + beta_R\n",
    "# beta_Female:R = log_loss_female_R - log_loss_female_Q - (log_loss_male_R - log_loss_male_Q)\n",
    "\n",
    "beta_Female_R = log_loss_female_R - log_loss_female_Q - (log_loss_male_R - log_loss_male_Q)\n",
    "beta_Female_R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3359.8199999999997"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RSS values at the nodes\n",
    "rss_root = 8430.00  # This is RSS_A, as it's the RSS at the root node if pruned\n",
    "rss_e = 1360.00  # RSS at node E\n",
    "rss_b_left = 204.00  # Left child of B\n",
    "rss_b_right = 7.36  # Right child of B\n",
    "rss_d_left = 139.00 # Left child of D\n",
    "\n",
    "# Total RSS when pruning node E is the sum of RSS of B's children and D's left child plus E's own RSS\n",
    "rss_prune_e = rss_b_left + rss_b_right + rss_d_left + rss_e\n",
    "\n",
    "# Number of terminal nodes if we prune E (B's children and D's left child)\n",
    "num_terminal_nodes_prune_e = 3\n",
    "\n",
    "# Now we want to find α such that the cost complexity of pruning E is less than that of pruning A\n",
    "# rss_prune_e + α * num_terminal_nodes_prune_e < rss_root + α\n",
    "# We isolate α to find the inequality\n",
    "# α < (rss_root - rss_prune_e) / (num_terminal_nodes_prune_e - 1)\n",
    "\n",
    "alpha_upper_bound = (rss_root - rss_prune_e) / (num_terminal_nodes_prune_e - 1)\n",
    "alpha_upper_bound\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.130379216946711"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficient for Education\n",
    "education_coefficient = 0.7563\n",
    "\n",
    "# Calculate the multiplicative increase in the odds\n",
    "odds_multiplier = np.exp(education_coefficient)\n",
    "odds_multiplier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best model is Model II with a minimized quantity of 121.98\n",
      "The predicted value using Model II's coefficients is: 53.83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Coefficients for the three models\n",
    "coefficients_model_I = [1.80, 2.63, 3.15]\n",
    "coefficients_model_II = [1.73, 2.51, 3.03]\n",
    "coefficients_model_III = [1.55, 2.21, 2.71]\n",
    "\n",
    "# Residual sum of squares for the three models\n",
    "rss_model_I = 82.10\n",
    "rss_model_II = 85.03\n",
    "rss_model_III = 100.91\n",
    "\n",
    "# Lambda for ridge regression penalty\n",
    "lambda_ridge = 2\n",
    "\n",
    "# Function to calculate the ridge regression quantity\n",
    "def ridge_quantity(rss, coefficients, lambda_ridge):\n",
    "    penalty = sum(coef**2 for coef in coefficients)\n",
    "    return rss + lambda_ridge * penalty\n",
    "\n",
    "# Calculate the quantity for each model\n",
    "quantity_I = ridge_quantity(rss_model_I, coefficients_model_I, lambda_ridge)\n",
    "quantity_II = ridge_quantity(rss_model_II, coefficients_model_II, lambda_ridge)\n",
    "quantity_III = ridge_quantity(rss_model_III, coefficients_model_III, lambda_ridge)\n",
    "\n",
    "# Find the model with the minimum quantity\n",
    "quantities = {'Model I': quantity_I, 'Model II': quantity_II, 'Model III': quantity_III}\n",
    "best_model = min(quantities, key=quantities.get)\n",
    "best_quantity = quantities[best_model]\n",
    "\n",
    "# Predicted value calculation for Model II (if required for specific inputs)\n",
    "# Assuming the inputs are provided or calculated somewhere else in the code\n",
    "inputs = [9, 8, 6]  # Example input values for the predictors\n",
    "predicted_value_Model_II = sum(coef * x for coef, x in zip(coefficients_model_II, inputs))\n",
    "\n",
    "print(f\"The best model is {best_model} with a minimized quantity of {best_quantity:.2f}\")\n",
    "print(f\"The predicted value using Model II's coefficients is: {predicted_value_Model_II:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle -\\infty < \\alpha \\wedge \\alpha < 232.54$"
      ],
      "text/plain": [
       "(-oo < alpha) & (alpha < 232.54)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, solve\n",
    "\n",
    "# Define the symbol for alpha\n",
    "alpha = symbols('alpha')\n",
    "\n",
    "# RSS and |T| for Strategy 1 and 2\n",
    "rss1 = 2050.42\n",
    "t1 = 6\n",
    "rss2 = 1817.88\n",
    "t2 = 7\n",
    "\n",
    "# Set up the inequality based on RSS + α|T|\n",
    "inequality = rss2 + t2 * alpha < rss1 + t1 * alpha\n",
    "\n",
    "# Solve for alpha\n",
    "alpha_value = solve(inequality, alpha)\n",
    "alpha_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound of the 90% prediction interval for x_28: 64.5733123278319\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Given parameters\n",
    "phi_0 = 5.1\n",
    "phi_1 = 0.9\n",
    "x_25 = 83\n",
    "MSE = 11.9\n",
    "n = 25\n",
    "\n",
    "# Predict x_28\n",
    "x_26 = phi_0 + phi_1 * x_25\n",
    "x_27 = phi_0 + phi_1 * x_26\n",
    "x_28 = phi_0 + phi_1 * x_27\n",
    "\n",
    "# Calculate mean of the observed values\n",
    "x_mean = np.mean([x_25, x_26, x_27])\n",
    "\n",
    "# Calculate the critical value from the t-distribution for 90% confidence level with n-2 degrees of freedom\n",
    "t_critical = t.ppf(0.95, df=n-2)\n",
    "\n",
    "# Calculate the lower bound of the prediction interval\n",
    "margin_of_error = t_critical * np.sqrt(MSE * (1 + 1/n + (x_28 - x_mean)**2 / np.sum([(xi - x_mean)**2 for xi in [x_25, x_26, x_27]])))\n",
    "lower_bound = x_28 - margin_of_error\n",
    "\n",
    "print(\"Lower bound of the 90% prediction interval for x_28:\", lower_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final prediction for the number of fish caught: 2.9004532595076915\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Coefficients from the count model portion\n",
    "intercept_count = 1.5979\n",
    "child_coef = -1.0428\n",
    "camper_yes_coef = 0.8340\n",
    "\n",
    "# Coefficients from the zero-inflated model portion\n",
    "intercept_zero = 1.2974\n",
    "people_coef = -0.5643\n",
    "\n",
    "# Input variables\n",
    "child = 1\n",
    "adults = 3\n",
    "camper = 1  # Assuming \"Yes\" for camper presence\n",
    "\n",
    "# Calculate predicted fish caught using the count model portion\n",
    "predicted_fish_count = np.exp(intercept_count) * np.exp(child_coef * child) * np.exp(camper_yes_coef * camper)\n",
    "\n",
    "# Calculate predicted excess probability of zero fish using the zero-inflated model portion\n",
    "predicted_excess_zero_prob = 1 / (1 + np.exp(-(intercept_zero + people_coef * (child + adults))))\n",
    "\n",
    "# Calculate final prediction for the number of fish caught\n",
    "final_prediction = predicted_fish_count * (1 - predicted_excess_zero_prob)\n",
    "\n",
    "print(\"Final prediction for the number of fish caught:\", final_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 2304.0, 4096.0, 6400.0, 2304.0, 227.55555555555554, 10.125)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the values\n",
    "n = 20\n",
    "sum_x = 100\n",
    "sum_y = 120\n",
    "sum_x2 = 2100\n",
    "sum_y2 = 7120\n",
    "sum_xy = 2520\n",
    "beta_1 = 1.2\n",
    "\n",
    "# Calculate mean x and mean y\n",
    "mean_x = sum_x / n\n",
    "mean_y = sum_y / n\n",
    "\n",
    "# Calculate beta_0 using the formula for the intercept\n",
    "beta_0 = mean_y - beta_1 * mean_x\n",
    "\n",
    "# Calculate SST, SSR, and SSE\n",
    "SST = sum_y2 - (sum_y**2 / n)\n",
    "SSR = beta_1**2 * (sum_x2 - (sum_x**2 / n))\n",
    "SSE = SST - SSR\n",
    "\n",
    "# Calculate MSR and MSE\n",
    "df_regression = 1\n",
    "df_error = n - 2\n",
    "\n",
    "MSR = SSR / df_regression\n",
    "MSE = SSE / df_error\n",
    "\n",
    "# Calculate the F-ratio\n",
    "F_ratio = MSR / MSE\n",
    "\n",
    "beta_0, SSR, SSE, SST, MSR, MSE, F_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following predictors should be removed: ['Temp', 'SeasonSpring', 'SeasonSummer']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Coefficient estimates and standard errors\n",
    "coefficients = {'Intercept': 27.3122, 'Police': 3.9362, 'Temp': 0.0381, 'Departure': -0.6090, 'SeasonSpring': -0.1147, 'SeasonSummer': 0.6896, 'SeasonWinter': 3.0978}\n",
    "standard_errors = {'Intercept': 2.3573, 'Police': 0.3266, 'Temp': 0.0365, 'Departure': 0.1331, 'SeasonSpring': 1.1055, 'SeasonSummer': 1.4019, 'SeasonWinter': 0.9913}\n",
    "\n",
    "# Degrees of freedom (assuming 100 observations)\n",
    "df = 100\n",
    "\n",
    "# Calculate t-statistics\n",
    "t_statistics = {predictor: coefficients[predictor] / standard_errors[predictor] for predictor in coefficients}\n",
    "\n",
    "# Critical value for 95% confidence level\n",
    "critical_value = 1.96\n",
    "\n",
    "# Check significance of predictors\n",
    "significant_predictors = [predictor for predictor, t_statistic in t_statistics.items() if abs(t_statistic) > critical_value]\n",
    "\n",
    "# Determine which predictors should be removed\n",
    "if len(significant_predictors) == len(coefficients):\n",
    "    print(\"None of the predictors should be removed.\")\n",
    "else:\n",
    "    print(\"The following predictors should be removed:\", [predictor for predictor in coefficients if predictor not in significant_predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following predictors should be removed: ['Temp', 'SeasonSpring', 'SeasonSummer']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Coefficient estimates and standard errors\n",
    "coefficients = {'Intercept': 27.3122, 'Police': 3.9362, 'Temp': 0.0381, 'Departure': -0.6090, 'SeasonSpring': -0.1147, 'SeasonSummer': 0.6896, 'SeasonWinter': 3.0978}\n",
    "standard_errors = {'Intercept': 2.3573, 'Police': 0.3266, 'Temp': 0.0365, 'Departure': 0.1331, 'SeasonSpring': 1.1055, 'SeasonSummer': 1.4019, 'SeasonWinter': 0.9913}\n",
    "\n",
    "# Calculate t-statistics\n",
    "t_statistics = {predictor: coefficients[predictor] / standard_errors[predictor] for predictor in coefficients}\n",
    "\n",
    "# Degrees of freedom (assuming 100 observations)\n",
    "df = 100\n",
    "\n",
    "# Calculate critical value for 95% confidence level\n",
    "critical_value = t.ppf(0.975, df)\n",
    "\n",
    "# Check significance of predictors\n",
    "significant_predictors = [predictor for predictor, t_statistic in t_statistics.items() if abs(t_statistic) > critical_value]\n",
    "\n",
    "# Determine which predictors should be removed\n",
    "if len(significant_predictors) == len(coefficients):\n",
    "    print(\"None of the predictors should be removed.\")\n",
    "else:\n",
    "    print(\"The following predictors should be removed:\", [predictor for predictor in coefficients if predictor not in significant_predictors])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following predictor(s) should be removed: Temp, SeasonSummer\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# Coefficients and standard errors for all predictors\n",
    "coefficients = {'Police': 3.9362, 'Temp': 0.0381, 'Departure': -0.6090, 'SeasonSummer': 0.6896}\n",
    "standard_errors = {'Police': 0.3266, 'Temp': 0.0365, 'Departure': 0.1331, 'SeasonSummer': 1.4019}\n",
    "\n",
    "# Calculate t-statistics for each predictor\n",
    "t_statistics = {predictor: coefficients[predictor] / standard_errors[predictor] for predictor in coefficients}\n",
    "\n",
    "# Degrees of freedom (assuming 100 observations)\n",
    "df = 100\n",
    "\n",
    "# Calculate critical value for 95% confidence level\n",
    "critical_value = t.ppf(0.975, df)\n",
    "\n",
    "# Determine which predictors should be removed based on their t-statistics\n",
    "remove_predictors = [predictor for predictor, t_statistic in t_statistics.items() if abs(t_statistic) < critical_value]\n",
    "\n",
    "# Print the predictor(s) to be removed\n",
    "if len(remove_predictors) == 0:\n",
    "    print(\"None of the predictors should be removed.\")\n",
    "else:\n",
    "    print(\"The following predictor(s) should be removed:\", ', '.join(remove_predictors))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Change in expected test mean squared error: 0\n"
     ]
    }
   ],
   "source": [
    "# Initial bias and variance\n",
    "B1 = 2\n",
    "V1 = 5\n",
    "\n",
    "# Final bias and variance\n",
    "B2 = 0\n",
    "V2 = 9\n",
    "\n",
    "# Calculate change in MSE\n",
    "delta_MSE = (B2**2 + V2) - (B1**2 + V1)\n",
    "\n",
    "print(\"Change in expected test mean squared error:\", delta_MSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sqrt(s_Y^2 - s^2): 1.063014581273465\n",
      "R-squared: 0.8355555555555556\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Residuals\n",
    "residuals = [0.4, -0.3, 0.0, -0.7]\n",
    "\n",
    "# Sample variance of the dependent variable\n",
    "s_Y_squared = 1.5\n",
    "\n",
    "# Number of observations\n",
    "n = len(residuals)\n",
    "\n",
    "# Number of predictors (including intercept)\n",
    "k = 2\n",
    "\n",
    "# Calculate the standard error of the estimate (s)\n",
    "s = np.sqrt(np.sum(np.square(residuals)) / (n - k))\n",
    "\n",
    "# Calculate sqrt(s_Y^2 - s^2)\n",
    "sqrt_s_Y_squared_minus_s_squared = np.sqrt(s_Y_squared - s**2)\n",
    "\n",
    "print(\"sqrt(s_Y^2 - s^2):\", sqrt_s_Y_squared_minus_s_squared)\n",
    "# Calculate sum of squares of residuals\n",
    "SS_res = np.sum(np.square(residuals))\n",
    "\n",
    "# Calculate total sum of squares\n",
    "SS_tot = (n - 1) * s_Y_squared\n",
    "\n",
    "# Calculate R-squared\n",
    "R_squared = 1 - (SS_res / SS_tot)\n",
    "\n",
    "print(\"R-squared:\", R_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.8, 39.199999999999996, 1.0, 61.0)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the points for each cluster as provided in the initial prompt\n",
    "points_a = np.array([[6,4], [6,5], [7,6], [8,6], [9,9]])\n",
    "points_b = np.array([[3,6], [3,9], [4,1], [5,5], [5,7]])\n",
    "points_c = np.array([[2,2], [3,1]])\n",
    "\n",
    "# Function to calculate the centroid of a cluster\n",
    "def calculate_centroid(points):\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "# Function to calculate the within-cluster sum of squared distances\n",
    "def within_cluster_variation(points, centroid):\n",
    "    return np.sum((points - centroid)**2)\n",
    "\n",
    "# Calculate centroids for each cluster\n",
    "centroid_a = calculate_centroid(points_a)\n",
    "centroid_b = calculate_centroid(points_b)\n",
    "centroid_c = calculate_centroid(points_c)\n",
    "\n",
    "# Calculate within-cluster variations\n",
    "variation_a = within_cluster_variation(points_a, centroid_a)\n",
    "variation_b = within_cluster_variation(points_b, centroid_b)\n",
    "variation_c = within_cluster_variation(points_c, centroid_c)\n",
    "\n",
    "# Calculate total within-cluster variation\n",
    "total_variation = variation_a + variation_b + variation_c\n",
    "variation_a, variation_b, variation_c, total_variation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "918.6602"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given values\n",
    "k = 6  # number of parameters in the model\n",
    "log_likelihood_2 = -457.5914  # log-likelihood of the second model\n",
    "likelihood_ratio_test_statistic = 8.5226\n",
    "\n",
    "# Calculate log-likelihood of the first model\n",
    "log_likelihood_1 = log_likelihood_2 + likelihood_ratio_test_statistic / 2\n",
    "\n",
    "# Calculate the AIC for the first model\n",
    "AIC = 2 * k - 2 * log_likelihood_1\n",
    "\n",
    "# Display the AIC for the first model\n",
    "AIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD_1 is less than -0.110 milligrams.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Given parameters\n",
    "beta_0 = -2.303  # Assuming a value of -2.303 for the intercept\n",
    "beta_1_lethal = 0.080\n",
    "beta_1_effective = 3.210\n",
    "therapeutic_index = 13\n",
    "\n",
    "# Probabilities\n",
    "p_lethal = 0.01\n",
    "p_effective = 0.99\n",
    "\n",
    "# Calculate ED_99\n",
    "logit_effective = np.log(p_effective / (1 - p_effective))\n",
    "ed_99 = (logit_effective - beta_0) / beta_1_effective\n",
    "\n",
    "# Calculate LD_1 upper bound\n",
    "logit_lethal = np.log(p_lethal / (1 - p_lethal))\n",
    "ld_1_upper = logit_lethal / (beta_1_effective * therapeutic_index)\n",
    "\n",
    "print(f\"LD_1 is less than {ld_1_upper:.3f} milligrams.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.33422909943493984,\n",
       " 1.7958848187036691,\n",
       " 0.11636363636363636,\n",
       " 0.96,\n",
       " -0.8311369656442076)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# Given values\n",
    "XtX_inv = np.array([[0.83, -0.67, -0.41, -0.45],\n",
    "                    [-0.67, 0.98, 0.11, 0.25],\n",
    "                    [-0.41, 0.11, 0.96, -0.23],\n",
    "                    [-0.45, 0.25, -0.23, 0.82]])\n",
    "\n",
    "XtY = np.array([6.90, 3.24, 3.20, 3.61])\n",
    "\n",
    "n = 15  # number of observations\n",
    "p = 3   # number of parameters\n",
    "SSE = 1.28  # sum of squares of error\n",
    "\n",
    "# Calculate estimates of beta\n",
    "beta_hat = XtX_inv.dot(XtY)\n",
    "beta_hat\n",
    "\n",
    "# Calculate estimate of variance\n",
    "sigma_squared = SSE / (n - p-1)\n",
    "\n",
    "\n",
    "# # Calculate the standard error for beta_2\n",
    "SE_beta_2 = np.sqrt(sigma_squared * XtX_inv[2, 2])\n",
    "\n",
    "# # Determine the t-score for 90% confidence level\n",
    "t_score = t.ppf(0.95, df=(n - p-1))\n",
    "\n",
    "# # Calculate the lower bound of the 90% confidence interval for beta_2\n",
    "lower_bound_beta_2 = beta_hat[2] - t_score * SE_beta_2\n",
    "\n",
    "\n",
    "SE_beta_2, t_score,sigma_squared,XtX_inv[2, 2], lower_bound_beta_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5619999999999998"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given principal component loading for BMI\n",
    "loading_BMI = 0.6\n",
    "\n",
    "# Calculate the loading for BFP knowing that the sum of squares of the loadings must be 1\n",
    "loading_BFP_squared = 1 - loading_BMI**2\n",
    "loading_BFP = -1 * (loading_BFP_squared)**0.5\n",
    "\n",
    "# Weightlifter's data\n",
    "BMI = 27.0\n",
    "BFP = 11.0\n",
    "\n",
    "# Calculate the means of each variable\n",
    "mean_BMI = 2623 / 100  # sum of all BMI values / number of weightlifters\n",
    "mean_BFP = 972 / 100   # sum of all BFP values / number of weightlifters\n",
    "\n",
    "# Calculate the first principal component score\n",
    "PC1_score = loading_BMI * (BMI - mean_BMI) + loading_BFP * (BFP - mean_BFP)\n",
    "PC1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32.090376,\n",
       " 33.215306,\n",
       " 35.709716,\n",
       " [0.23962399999999917, 1.6646940000000043, 1.0102839999999986],\n",
       " 0.9715340000000007)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given data\n",
    "y_27 = 31.18\n",
    "y_28 = 32.33\n",
    "y_29 = 34.88\n",
    "y_30 = 36.72  # Actual value of y_30, not used in predictions\n",
    "\n",
    "# Coefficients from the AR(1) model\n",
    "intercept = 1.5901\n",
    "slope = 0.9782\n",
    "\n",
    "# Predictions\n",
    "y_28_pred = intercept + slope * y_27\n",
    "y_29_pred = intercept + slope * y_28\n",
    "y_30_pred = intercept + slope * y_29\n",
    "\n",
    "# Actual values for validation\n",
    "actual_values = [y_28, y_29, y_30]\n",
    "predictions = [y_28_pred, y_29_pred, y_30_pred]\n",
    "\n",
    "# Calculate errors\n",
    "errors = [actual - pred for actual, pred in zip(actual_values, predictions)]\n",
    "mean_error = sum(errors) / len(errors)\n",
    "\n",
    "y_28_pred, y_29_pred, y_30_pred, errors, mean_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error: 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given time series\n",
    "t = [21, 22, 23, 24, 25, 26, 27, 28, 29, 30]\n",
    "y = [27.95, 29.44, 30.54, 29.49, 29.89, 30.32, 31.18, 32.33, 34.88, 36.72]\n",
    "\n",
    "# Fitted AR(1) model\n",
    "a = 1.5901\n",
    "b = 0.9782\n",
    "\n",
    "# Validation subsample\n",
    "validation_t = t[-3:]\n",
    "validation_y = y[-3:]\n",
    "\n",
    "# Generate forecasts for validation subsample\n",
    "validation_forecasts = []\n",
    "for i in range(len(validation_t)):\n",
    "    if i == 0:\n",
    "        validation_forecasts.append(a + b * y[-4])\n",
    "    else:\n",
    "        validation_forecasts.append(a + b * y[t.index(validation_t[i-1])])\n",
    "\n",
    "# Calculate errors\n",
    "errors = [validation_y[i] - validation_forecasts[i] for i in range(len(validation_y))]\n",
    "\n",
    "# Calculate mean error\n",
    "mean_error = np.mean(errors)\n",
    "\n",
    "print(f\"Mean error: {mean_error:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast for y_12: 204.00\n",
      "Standard deviation of noise terms: 4.37\n",
      "Standard error for 4 step-ahead forecast: 8.7\n",
      "Standard error for 6 step-ahead forecast: 10.7\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data points\n",
    "y = np.array([200, 197, 205, 206, 203, 205, 202])\n",
    "\n",
    "# Forecast horizon for y_12 from the last observed point y_6\n",
    "forecast_horizon_y12 = 6\n",
    "\n",
    "changes = np.diff(y)\n",
    "\n",
    "# Calculate the average change\n",
    "average_change = np.mean(changes)\n",
    "\n",
    "# Calculate the forecast for y_12 by adding the average change to the last observed value y_6\n",
    "forecast_y12 = y[-1] + average_change * forecast_horizon_y12\n",
    "\n",
    "# Estimate the standard deviation of changes (noise terms)\n",
    "sigma = np.std(changes, ddof=1)\n",
    "\n",
    "# Calculate standard errors for forecasts 4 steps and 6 steps ahead\n",
    "se_4_steps = sigma * np.sqrt(4)\n",
    "se_6_steps = sigma * np.sqrt(6)\n",
    "\n",
    "print(f\"Forecast for y_12: {forecast_y12:.2f}\")\n",
    "print(f\"Standard deviation of noise terms: {sigma:.2f}\")\n",
    "print(f\"Standard error for 4 step-ahead forecast: {se_4_steps:.1f}\")\n",
    "print(f\"Standard error for 6 step-ahead forecast: {se_6_steps:.1f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forecast for y_12: 204.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data points\n",
    "y = np.array([200, 197, 205, 206, 203, 205, 202])\n",
    "\n",
    "# Forecast horizon for y_12 from the last observed point y_6\n",
    "forecast_horizon_y12 = 6\n",
    "\n",
    "# Calculate changes (assumed to be the noise terms in random walk)\n",
    "changes = np.diff(y)\n",
    "\n",
    "# Calculate the average change\n",
    "average_change = np.mean(changes)\n",
    "\n",
    "# Calculate the forecast for y_12 by adding the average change to the last observed value y_6\n",
    "forecast_y12 = y[-1] + average_change * forecast_horizon_y12\n",
    "\n",
    "print(f\"Forecast for y_12: {forecast_y12:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability that Employee B has no disability: 0.059873986058751334\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Given coefficients\n",
    "beta_1 = 0.02\n",
    "beta_2 = 0.3\n",
    "\n",
    "# Age and years with the company for Employee B\n",
    "age_B = 25\n",
    "years_B = 5\n",
    "\n",
    "# Calculating alpha_1 using the predicted probability for Employee A\n",
    "p_A = 0.32\n",
    "alpha_1 = 4 - math.log(1/p_A - 1) - 0.02 * 50 - 0.3 * 10\n",
    "\n",
    "# Calculating the predicted probability for Employee B\n",
    "logit_p_B = alpha_1 - beta_1 * age_B - beta_2 * years_B\n",
    "p_B = 1 / (1 + math.exp(-logit_p_B))\n",
    "\n",
    "print(\"Predicted probability that Employee B has no disability:\", p_B)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: 2.2099447513812156\n",
      "Critical value for alpha=0.01: 3.169272667175838\n",
      "Critical value for alpha=0.02: 2.763769457447889\n",
      "Critical value for alpha=0.05: 2.2281388519649385\n",
      "Critical value for alpha=0.1: 1.8124611228107335\n",
      "At alpha=0.01, do not reject the null hypothesis\n",
      "At alpha=0.02, do not reject the null hypothesis\n",
      "At alpha=0.05, do not reject the null hypothesis\n",
      "At alpha=0.1, reject the null hypothesis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Given values\n",
    "beta_1_hat = 0.40\n",
    "beta_1_null = 0\n",
    "SE_beta_1 = 0.181\n",
    "alpha_values = [0.01, 0.02, 0.05, 0.1]\n",
    "degrees_of_freedom = 12 - 2  # 12 observations, 2 parameters estimated\n",
    "\n",
    "# Calculate t-statistic\n",
    "t_statistic = (beta_1_hat - beta_1_null) / SE_beta_1\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "\n",
    "# Find critical values\n",
    "for alpha in alpha_values:\n",
    "    critical_value = t.ppf(1 - alpha / 2, degrees_of_freedom)\n",
    "    print(f\"Critical value for alpha={alpha}: {critical_value}\")\n",
    "\n",
    "# Compare t-statistic with critical values\n",
    "for alpha in alpha_values:\n",
    "    critical_value = t.ppf(1 - alpha / 2, degrees_of_freedom)\n",
    "    if abs(t_statistic) > critical_value:\n",
    "        print(f\"At alpha={alpha}, reject the null hypothesis\")\n",
    "    else:\n",
    "        print(f\"At alpha={alpha}, do not reject the null hypothesis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: 2.2099447513812156\n",
      "Critical value for alpha=0.01: 3.169272667175838\n",
      "Critical value for alpha=0.02: 2.763769457447889\n",
      "Critical value for alpha=0.05: 2.2281388519649385\n",
      "Critical value for alpha=0.1: 1.8124611228107335\n",
      "At alpha=0.01, do not reject the null hypothesis\n",
      "At alpha=0.02, do not reject the null hypothesis\n",
      "At alpha=0.05, do not reject the null hypothesis\n",
      "At alpha=0.1, reject the null hypothesis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Given values\n",
    "beta_0_hat = 15.52\n",
    "beta_1_hat = 0.40\n",
    "beta_1_null = 0\n",
    "SE_beta_1 = 0.181\n",
    "alpha_values = [0.01, 0.02, 0.05, 0.1]\n",
    "degrees_of_freedom = 12 - 2  # 12 observations, 2 parameters estimated\n",
    "\n",
    "# Calculate t-statistic\n",
    "t_statistic = (beta_1_hat - beta_1_null) / SE_beta_1\n",
    "print(\"t-statistic:\", t_statistic)\n",
    "\n",
    "# Find critical values\n",
    "for alpha in alpha_values:\n",
    "    critical_value = t.ppf(1 - alpha / 2, degrees_of_freedom)\n",
    "    print(f\"Critical value for alpha={alpha}: {critical_value}\")\n",
    "\n",
    "# Compare t-statistic with critical values\n",
    "for alpha in alpha_values:\n",
    "    critical_value = t.ppf(1 - alpha / 2, degrees_of_freedom)\n",
    "    if abs(t_statistic) > critical_value:\n",
    "        print(f\"At alpha={alpha}, reject the null hypothesis\")\n",
    "    else:\n",
    "        print(f\"At alpha={alpha}, do not reject the null hypothesis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of coefficients not statistically different from zero: 3\n"
     ]
    }
   ],
   "source": [
    "# Given values\n",
    "t_statistics = [1.661, -0.565, 5.000, -0.301, -2.090]\n",
    "alpha = 0.10\n",
    "degrees_of_freedom = 20 - 1  # degrees of freedom for multiple regression with 20 observations\n",
    "\n",
    "# Critical value for two-tailed test\n",
    "critical_value = abs(t.ppf(alpha/2, degrees_of_freedom))\n",
    "\n",
    "# Count coefficients not statistically different from zero\n",
    "count_not_different = sum(abs(t_stat) < critical_value for t_stat in t_statistics)\n",
    "\n",
    "print(\"Number of coefficients not statistically different from zero:\", count_not_different)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Obs. |   yi  | xi,1 | xi,2 |\n",
      "|------|-------|------|------|\n",
      "| 1     | 7.1   | 1     | 11    |\n",
      "| 2     | 11.3  | 4     | 9     |\n",
      "| 3     | 18.1  | 7     | 3     |\n",
      "| 4     | 18.6  | 8     | 4     |\n"
     ]
    }
   ],
   "source": [
    "# Define the data\n",
    "X = np.array([[1, 1, 11], [1, 4, 9], [1, 7, 3], [1, 8, 4]])\n",
    "y = np.array([7.1, 11.3, 18.1, 18.6])\n",
    "\n",
    "# Define coefficients for Model I\n",
    "beta0_model1 = 15.658\n",
    "beta1_model1 = 0.689\n",
    "beta2_model1 = -0.789\n",
    "\n",
    "\n",
    "# Define coefficients for Model 2\n",
    "beta0_model2 = 20.325\n",
    "beta1_model2 = 0\n",
    "beta2_model2 = -0.970\n",
    "\n",
    "# Define the values of x1 and x2\n",
    "x1 = 8\n",
    "x2 = 8\n",
    "\n",
    "# Calculate the predicted value for Model I\n",
    "predicted_value_model1 = beta0_model1 + beta1_model1 * x1 + beta2_model1 * x2\n",
    "\n",
    "# Print the result\n",
    "print(\"Predicted value for Model I:\", predicted_value_model1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestClassifier(random_state=0)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=0)\n",
    "X = [[ 1,  2,  3],  # 2 samples, 3 features\n",
    "     [11, 12, 13]]\n",
    "y = [0, 1]  # classes of each sample\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(X)  # predict classes of the training data\n",
    "clf.predict([[4, 5, 6], [14, 15, 16]])  # predict classes of new data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen model: Model I\n",
      "Predicted value: 14.857999999999997\n"
     ]
    }
   ],
   "source": [
    "# Define the data\n",
    "X = np.array([[1, 1, 11], [1, 4, 9], [1, 7, 3], [1, 8, 4]])\n",
    "y = np.array([7.1, 11.3, 18.1, 18.6])\n",
    "\n",
    "# Define lambda\n",
    "lmbda = 8.15\n",
    "\n",
    "# Define coefficients for Model I\n",
    "beta0_model1 = 15.658\n",
    "beta1_model1 = 0.689\n",
    "beta2_model1 = -0.789\n",
    "\n",
    "# Define coefficients for Model II\n",
    "beta0_model2 = 20.325\n",
    "beta1_model2 = 0\n",
    "beta2_model2 = -0.970\n",
    "\n",
    "# Calculate the Lasso objective function for Model I\n",
    "lasso_obj_model1 = np.sum((y - (beta0_model1 + beta1_model1 * X[:,1] + beta2_model1 * X[:,2]))**2) + lmbda * (np.abs(beta1_model1) + np.abs(beta2_model1))\n",
    "\n",
    "# Calculate the Lasso objective function for Model II\n",
    "lasso_obj_model2 = np.sum((y - (beta0_model2 + beta1_model2 * X[:,1] + beta2_model2 * X[:,2]))**2) + lmbda * (np.abs(beta1_model2) + np.abs(beta2_model2))\n",
    "\n",
    "# Choose the model with the smaller Lasso objective function\n",
    "chosen_model = 1 if lasso_obj_model1 < lasso_obj_model2 else 2\n",
    "\n",
    "# Calculate the predicted value using the chosen model\n",
    "if chosen_model == 1:\n",
    "    predicted_value = beta0_model1 + beta1_model1 * 8 + beta2_model1 * 8\n",
    "    print(\"Chosen model: Model I\")\n",
    "else:\n",
    "    predicted_value = beta0_model2 + beta1_model2 * 8 + beta2_model2 * 8\n",
    "    print(\"Chosen model: Model II\")\n",
    "\n",
    "# Print the predicted value\n",
    "print(\"Predicted value:\", predicted_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3508771929824563"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Defining the correlation matrix\n",
    "correlation_matrix = np.array([[1, 0.65, 0.4],\n",
    "                               [0.65, 1, 0.5],\n",
    "                               [0.4, 0.5, 1]])\n",
    "\n",
    "# Calculating the inverse of the correlation matrix\n",
    "inverse_correlation_matrix = np.linalg.inv(correlation_matrix)\n",
    "\n",
    "# Variance Inflation Factor (VIF) for x_2\n",
    "# VIF for x_2 is the diagonal element corresponding to x_2 in the inverse correlation matrix\n",
    "vif_x2 = inverse_correlation_matrix[2, 2]\n",
    "vif_x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6197, -0.1933, -0.2309, -0.0708])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_hat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11636363636363636"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigma_squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound of 90% CI for beta_2: 110.450\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "import numpy as np\n",
    "\n",
    "# Given values\n",
    "XtX_inv = np.array([[0.83, -0.67, -0.41, -0.45],\n",
    "                    [-0.67, 0.98, 0.11, 0.25],\n",
    "                    [-0.41, 0.11, 0.96, -0.23],\n",
    "                    [-0.45, 0.25, -0.23, 0.82]])\n",
    "\n",
    "XtY = np.array([6.90, 3.24, 3.20, 3.61])\n",
    "\n",
    "n = 15  # number of observations\n",
    "p = 4   # number of parameters\n",
    "SSE = 1.28  # sum of squares of error\n",
    "\n",
    "# Step 1: Calculate estimates of beta\n",
    "beta_hat = np.linalg.inv(XtX_inv).dot(XtY)\n",
    "\n",
    "# Step 2: Calculate estimate of variance\n",
    "sigma_squared = SSE / (n - p)\n",
    "\n",
    "# Step 3: Calculate the standard error for beta_2\n",
    "SE_beta_2 = np.sqrt(sigma_squared * XtX_inv[1, 1])\n",
    "\n",
    "# Step 4: Determine the t-score for 90% confidence level\n",
    "alpha = 0.10  # 90% confidence level\n",
    "df = n - p\n",
    "t_score = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# Step 5: Calculate the lower bound of the 90% confidence interval for beta_2\n",
    "lower_bound_beta_2 = beta_hat[1] - t_score * SE_beta_2\n",
    "\n",
    "print(f\"Lower bound of 90% CI for beta_2: {lower_bound_beta_2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lower bound of 90% CI for beta_2: -3.15\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Given values\n",
    "beta_hat_2 = -2.29\n",
    "SE_beta_2 = 0.48\n",
    "n = 15\n",
    "p = 4\n",
    "\n",
    "# Step 1: Calculate degrees of freedom\n",
    "df = n - p\n",
    "\n",
    "# Step 2: Find the t-score for a 90% confidence level\n",
    "alpha = 0.10  # 90% confidence level\n",
    "t_score = t.ppf(1 - alpha/2, df)\n",
    "\n",
    "# Step 3: Calculate the lower bound\n",
    "lower_bound_beta_2 = beta_hat_2 - t_score * SE_beta_2\n",
    "\n",
    "print(f\"Lower bound of 90% CI for beta_2: {lower_bound_beta_2:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modeled estimate of the intercept parameter: [20.93037975]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data for matrices\n",
    "X = np.array([[1, 0, 1, 9],\n",
    "              [1, 1, 1, 15],\n",
    "              [1, 1, 1, 8],\n",
    "              [0, 1, 1, 7],\n",
    "              [0, 1, 1, 6],\n",
    "              [0, 0, 1, 6]])  # Sample data for X\n",
    "\n",
    "y = np.array([[21],\n",
    "              [32],\n",
    "              [19],\n",
    "              [17],\n",
    "              [15],\n",
    "              [15]])  # Sample data for y\n",
    "\n",
    "XtX_inv = np.linalg.inv(np.dot(X.T, X))  # Calculate (X^TX)^-1\n",
    "\n",
    "XtX_inv_Xty = np.dot(XtX_inv, np.dot(X.T, y))  # Calculate (X^TX)^-1X^Ty\n",
    "\n",
    "X_XtX_inv_Xty = np.dot(X, XtX_inv_Xty)  # Calculate X(X^TX)^-1X^Ty\n",
    "\n",
    "# Modeled estimate of the intercept parameter\n",
    "intercept_param = X_XtX_inv_Xty[0]  # First element of the column vector X(X^TX)^-1X^Ty\n",
    "\n",
    "print(\"Modeled estimate of the intercept parameter:\", intercept_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20.93037975],\n",
       "       [32.02531646],\n",
       "       [19.0443038 ],\n",
       "       [16.89240506],\n",
       "       [15.03797468],\n",
       "       [15.06962025]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_XtX_inv_Xty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Data:\n",
      "{'Obs.': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20], 'yi': [21.97, 13.16, 28.13, 18.61, 23.74, 32.96, 9.03, 20.14, 32.51, 8.45, 6.9, 19.27, 29.5, 33.6, 17.06, 20.89, 33.21, 18.49, 29.56, 16.94], 'xi1': [7, 12, 11, 7, 14, 14, 6, 13, 10, 10, 11, 7, 13, 7, 9, 14, 15, 7, 9, 6], 'xi2': [13, 8, 17, 3, 7, 10, 3, 7, 19, 3, 0, 3, 16, 17, 10, 13, 17, 6, 13, 3]}\n",
      "Estimated value of beta2: 0.975686113393591\n"
     ]
    }
   ],
   "source": [
    "# Define the combined dataset\n",
    "combined_data = {\n",
    "    \"Obs.\": list(range(1, 21)),\n",
    "    \"yi\": [21.97, 13.16, 28.13, 18.61, 23.74, 32.96, 9.03, 20.14, 32.51, 8.45,\n",
    "           6.90, 19.27, 29.50, 33.60, 17.06, 20.89, 33.21, 18.49, 29.56, 16.94],\n",
    "    \"xi1\": [7, 12, 11, 7, 14, 14, 6, 13, 10, 10,\n",
    "            11, 7, 13, 7, 9, 14, 15, 7, 9, 6],\n",
    "    \"xi2\": [13, 8, 17, 3, 7, 10, 3, 7, 19, 3,\n",
    "            0, 3, 16, 17, 10, 13, 17, 6, 13, 3]\n",
    "}\n",
    "\n",
    "# Print the combined dataset\n",
    "print(\"Combined Data:\")\n",
    "print(combined_data)\n",
    "\n",
    "# Given values\n",
    "sum_x2 = 2434\n",
    "sum_x1y = 4541.56\n",
    "sum_x1 = 202\n",
    "sum_x1x2 = 2014\n",
    "beta0 = 9.53\n",
    "beta1 = 0.12\n",
    "\n",
    "# Calculate beta2\n",
    "beta2 = (sum_x1y - beta0 * sum_x1 - beta1 * sum_x1x2) / sum_x2\n",
    "\n",
    "print(\"Estimated value of beta2:\", beta2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 6.702127659574468\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given information\n",
    "ssr = 7\n",
    "sst = 54\n",
    "n = 47\n",
    "\n",
    "# Calculate SSE\n",
    "sse = sst - ssr\n",
    "\n",
    "# Calculate F-statistic\n",
    "k = 1  # number of explanatory variables\n",
    "f_stat = (ssr/k) / (sse/(n-k-1))\n",
    "\n",
    "print(\"F-statistic:\", f_stat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for the Slope:\n",
      "Lower Bound: -1.6944825401567485\n",
      "Upper Bound: 17.99448254015675\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import t\n",
    "\n",
    "# Given values\n",
    "slope_estimate = 8.15\n",
    "intercept_estimate = 13.237\n",
    "sample_variance = 17.3\n",
    "residual_std_error = 28.77\n",
    "sample_size = 5\n",
    "df = sample_size - 2  # degrees of freedom\n",
    "\n",
    "# Calculate standard error of the slope estimate\n",
    "standard_error_slope = np.sqrt(residual_std_error ** 2 / (sample_size * sample_variance))\n",
    "\n",
    "# Calculate t-value for 95% confidence interval\n",
    "t_value = t.ppf(0.975, df)\n",
    "\n",
    "# Calculate margin of error\n",
    "margin_of_error = t_value * standard_error_slope\n",
    "\n",
    "# Calculate lower and upper bounds of the confidence interval\n",
    "lower_bound = slope_estimate - margin_of_error\n",
    "upper_bound = slope_estimate + margin_of_error\n",
    "\n",
    "print(\"95% Confidence Interval for the Slope:\")\n",
    "print(\"Lower Bound:\", lower_bound)\n",
    "print(\"Upper Bound:\", upper_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best split: X = 40.5\n",
      "Means: Below = 117.5, Above = 500.0\n",
      "Predicted value for X = 35: 117.5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "X = np.array([37, 32, 42, 37, 39])\n",
    "Y = np.array([190, 135, 500, 70, 75])\n",
    "\n",
    "# Function to calculate Mean Squared Error\n",
    "def mse(values):\n",
    "    if len(values) == 0:\n",
    "        return 0\n",
    "    mean = np.mean(values)\n",
    "    return np.mean((values - mean) ** 2)\n",
    "\n",
    "# Function to find the best split\n",
    "def find_best_split(X, Y):\n",
    "    unique_x = np.sort(np.unique(X))\n",
    "    best_mse = float('inf')\n",
    "    best_split = None\n",
    "    best_means = None\n",
    "\n",
    "    for i in range(len(unique_x) - 1):\n",
    "        split_point = (unique_x[i] + unique_x[i + 1]) / 2\n",
    "        left_mask = X < split_point\n",
    "        right_mask = X >= split_point\n",
    "        \n",
    "        left_y = Y[left_mask]\n",
    "        right_y = Y[right_mask]\n",
    "        \n",
    "        left_mse = mse(left_y)\n",
    "        right_mse = mse(right_y)\n",
    "        \n",
    "        total_mse = (left_mse * len(left_y) + right_mse * len(right_y)) / len(Y)\n",
    "        \n",
    "        if total_mse < best_mse:\n",
    "            best_mse = total_mse\n",
    "            best_split = split_point\n",
    "            best_means = (np.mean(left_y), np.mean(right_y))\n",
    "\n",
    "    return best_split, best_means\n",
    "\n",
    "# Find the best split\n",
    "split, means = find_best_split(X, Y)\n",
    "\n",
    "# Predict value for X=35\n",
    "prediction = means[0] if 35 < split else means[1]\n",
    "\n",
    "print(f\"Best split: X = {split}\")\n",
    "print(f\"Means: Below = {means[0]}, Above = {means[1]}\")\n",
    "print(f\"Predicted value for X = 35: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of x that maximizes the entropy is: 240\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "# Function to calculate entropy given the number of policyholders in each class\n",
    "def entropy(n_standard):\n",
    "    # Fixed number of policyholders in the Preferred class\n",
    "    n_preferred = 320\n",
    "    # Total number of policyholders\n",
    "    n_total = 800\n",
    "    # The number in the Substandard class is the remainder\n",
    "    n_substandard = n_total - n_preferred - n_standard\n",
    "    \n",
    "    # Probabilities for each class\n",
    "    p_preferred = n_preferred / n_total\n",
    "    p_standard = n_standard / n_total\n",
    "    p_substandard = n_substandard / n_total\n",
    "    \n",
    "    # Components of entropy, considering only cases where the probability is not zero to avoid log(0)\n",
    "    entropy_components = []\n",
    "    if p_preferred > 0:\n",
    "        entropy_components.append(p_preferred * np.log2(p_preferred))\n",
    "    if p_standard > 0:\n",
    "        entropy_components.append(p_standard * np.log2(p_standard))\n",
    "    if p_substandard > 0:\n",
    "        entropy_components.append(p_substandard * np.log2(p_substandard))\n",
    "    \n",
    "    # Summing the components and taking the negative gives the entropy\n",
    "    total_entropy = -sum(entropy_components)\n",
    "    \n",
    "    return total_entropy\n",
    "\n",
    "# Objective function to maximize entropy\n",
    "def objective_function(n_standard):\n",
    "    return -entropy(n_standard)\n",
    "\n",
    "# Optimize to find the value of n_standard that maximizes entropy\n",
    "result = minimize_scalar(objective_function, bounds=(0, 800-320), method='bounded')\n",
    "n_standard_optimal = int(result.x)\n",
    "\n",
    "print(f\"The value of x that maximizes the entropy is: {n_standard_optimal}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.,  1.,  5.,  8., 34.],\n",
       "        [ 1.,  0.,  2.,  5., 25.],\n",
       "        [ 5.,  2.,  0.,  1., 13.],\n",
       "        [ 8.,  5.,  1.,  0., 10.],\n",
       "        [34., 25., 13., 10.,  0.]]),\n",
       " array([[ 0.,  9.,  5.,  5.],\n",
       "        [ 9.,  0., 20.,  8.],\n",
       "        [ 5., 20.,  0.,  4.],\n",
       "        [ 5.,  8.,  4.,  0.]]),\n",
       " array([[0., 2., 5.],\n",
       "        [2., 0., 1.],\n",
       "        [5., 1., 0.]]),\n",
       " 41.6,\n",
       " 25.5,\n",
       " 5.333333333333333,\n",
       " 72.43333333333332)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the points in each cluster\n",
    "cluster_A = np.array([[6, 4], [6, 5], [7, 6], [8, 6], [9, 9]])\n",
    "cluster_B = np.array([[3, 6], [3, 9], [5, 5], [5, 7]])\n",
    "cluster_C = np.array([[2, 2], [3, 1], [4, 1]])\n",
    "\n",
    "# Function to calculate the centroid\n",
    "def calculate_centroid(points):\n",
    "    return np.mean(points, axis=0)\n",
    "\n",
    "# Calculate the centroids for each cluster\n",
    "centroid_A = calculate_centroid(cluster_A)\n",
    "centroid_B = calculate_centroid(cluster_B)\n",
    "centroid_C = calculate_centroid(cluster_C)\n",
    "\n",
    "centroid_A, centroid_B, centroid_C\n",
    "def squared_euclidean_distance(point1, point2):\n",
    "    return np.sum((point1 - point2) ** 2)\n",
    "\n",
    "# Function to generate distance matrix for a cluster\n",
    "def distance_matrix(cluster):\n",
    "    n = len(cluster)\n",
    "    distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            distances[i, j] = squared_euclidean_distance(cluster[i], cluster[j])\n",
    "    return distances\n",
    "\n",
    "# Generate distance matrices for each cluster\n",
    "distance_matrix_A = distance_matrix(cluster_A)\n",
    "distance_matrix_B = distance_matrix(cluster_B)\n",
    "distance_matrix_C = distance_matrix(cluster_C)\n",
    "\n",
    "# Function to calculate within-cluster variation\n",
    "def within_cluster_variation(dist_matrix):\n",
    "    return np.sum(dist_matrix) / len(dist_matrix)\n",
    "\n",
    "# Calculate within-cluster variation for each cluster\n",
    "variation_A = within_cluster_variation(distance_matrix_A)\n",
    "variation_B = within_cluster_variation(distance_matrix_B)\n",
    "variation_C = within_cluster_variation(distance_matrix_C)\n",
    "\n",
    "# Total within-cluster variation\n",
    "total_variation = variation_A + variation_B + variation_C\n",
    "\n",
    "distance_matrix_A, distance_matrix_B, distance_matrix_C, variation_A, variation_B, variation_C, total_variation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{ED99: 4.23513350242819, LD1: 55.0567355315665, beta_0: -8.99965869265991}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define the symbols\n",
    "LD1, ED99 = sp.symbols('LD1 ED99')\n",
    "\n",
    "# Given probabilities\n",
    "p_LD1 = 0.01\n",
    "p_ED99 = 0.99\n",
    "\n",
    "# Coefficients from the linear model\n",
    "beta_LD1 = 0.080\n",
    "beta_ED99 = 3.210\n",
    "\n",
    "# Set up the equations based on the logit model\n",
    "eq1 = sp.Eq(sp.ln(p_LD1 / (1 - p_LD1)), beta_LD1 * LD1)\n",
    "eq2 = sp.Eq(sp.ln(p_ED99 / (1 - p_ED99)), beta_ED99 * ED99)\n",
    "\n",
    "# Solve the simultaneous equations\n",
    "solution = sp.solve((eq1, eq2), (LD1, ED99))\n",
    "solution\n",
    "# New information: the intercepts of both models are equal, and LD1/ED99 = 13.\n",
    "\n",
    "# Define the intercept symbol\n",
    "beta_0 = sp.symbols('beta_0')\n",
    "\n",
    "# Set up the new equations with the common intercept beta_0\n",
    "eq1 = sp.Eq(sp.ln(p_LD1 / (1 - p_LD1)), beta_0 + beta_LD1 * LD1)\n",
    "eq2 = sp.Eq(sp.ln(p_ED99 / (1 - p_ED99)), beta_0 + beta_ED99 * ED99)\n",
    "\n",
    "# Additional information equation LD1 = 13 * ED99\n",
    "eq3 = sp.Eq(LD1, 13 * ED99)\n",
    "\n",
    "# Solve the simultaneous equations\n",
    "solution_with_intercept = sp.solve((eq1, eq2, eq3), (LD1, ED99, beta_0))\n",
    "solution_with_intercept\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.10400000000000001, 0.16952, 0.4117280655966994)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given inverse matrix\n",
    "inv_XTX = np.array([\n",
    "    [0.55, -0.16, -0.28, -0.32, -0.19],\n",
    "    [-0.16, 0.59, -0.20, -0.12, 0.02],\n",
    "    [-0.28, -0.20, 0.67, 0.15, -0.10],\n",
    "    [-0.32, -0.12, 0.15, 0.83, -0.18],\n",
    "    [-0.19, 0.02, -0.10, -0.18, 0.55]\n",
    "])\n",
    "\n",
    "# Vector a for the combination β1 + β2 - β3\n",
    "a = np.array([0, 1, 1, -1, 0])\n",
    "\n",
    "# Calculating MSE\n",
    "SSE = 1.56\n",
    "n = 20\n",
    "p = 5\n",
    "MSE = SSE / (n - p)\n",
    "\n",
    "# Calculating variance of the linear combination\n",
    "var_combination = a.T @ inv_XTX @ a * MSE\n",
    "\n",
    "# Standard error\n",
    "SE = np.sqrt(var_combination)\n",
    "MSE, var_combination, SE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of times the last observation is in the OOB samples: 183.6327278873813\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Parameters\n",
    "n = 300  # number of observations\n",
    "b = 500  # number of bootstrap samples (trees)\n",
    "\n",
    "# Probability that a specific observation is NOT chosen in a single bootstrap sample\n",
    "p_not_chosen = (1 - 1/n)**n\n",
    "\n",
    "# Expected number of times an observation is in the OOB samples across all trees\n",
    "expected_oob = b * p_not_chosen\n",
    "\n",
    "print(\"Expected number of times the last observation is in the OOB samples:\", expected_oob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability that the first predictor is not included in a split: 0.75\n"
     ]
    }
   ],
   "source": [
    "from math import comb\n",
    "\n",
    "# Parameters\n",
    "p = 16  # total number of predictors\n",
    "m = 4   # number of predictors considered at each split\n",
    "\n",
    "# Calculating combinations\n",
    "total_comb = comb(p, m)      # Total ways to choose m predictors from p\n",
    "comb_without_first = comb(p-1, m)  # Ways to choose m predictors from p-1 (excluding the first)\n",
    "\n",
    "# Probability that the first predictor is not included\n",
    "probability_not_included = comb_without_first / total_comb\n",
    "\n",
    "print(\"Probability that the first predictor is not included in a split:\", probability_not_included)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{a: 0.609425435765010, b: 0.804174736389068}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import symbols, Eq, solve\n",
    "\n",
    "# Defining the symbols\n",
    "a, b = symbols('a b')\n",
    "\n",
    "# Equation 1 based on Observation 1\n",
    "eq1 = Eq(6.09*a - 3.21*b, 1.13)\n",
    "\n",
    "# Equation 2 based on Observation 2\n",
    "eq2 = Eq(2.07*a - 1.32*b, 0.20)\n",
    "\n",
    "# Solving the equations\n",
    "solution = solve((eq1, eq2), (a, b))\n",
    "solution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle -2.13937163761567$"
      ],
      "text/plain": [
       "-2.13937163761567"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Values for Observation 3\n",
    "X1_obs3 = 5.98\n",
    "X2_obs3 = 11.14\n",
    "X1_mean = 7.3\n",
    "X2_mean = 12.8\n",
    "\n",
    "# Coefficients a and b\n",
    "a_val = solution[a]\n",
    "b_val = solution[b]\n",
    "\n",
    "# Calculate Z1 for Observation 3\n",
    "Z1_obs3 = a_val * (X1_obs3 - X1_mean) + b_val * (X2_obs3 - X2_mean)\n",
    "Z1_obs3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19379999999999997"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Proportion of variance explained by PC1\n",
    "variance_PC1 = 0.8062\n",
    "\n",
    "# MSE is the variance not captured by the first principal component\n",
    "mse_one_dimensional = 1 - variance_PC1\n",
    "mse_one_dimensional\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.400367627183861"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Coordinates of Observation 5\n",
    "x1_obs5, x2_obs5 = 5, 1\n",
    "\n",
    "# Coordinates of the centroid of Cluster 2\n",
    "x1_centroid2, x2_centroid2 = 4.000, 4.250\n",
    "\n",
    "# Calculating the Euclidean distance\n",
    "distance = math.sqrt((x1_centroid2 - x1_obs5)**2 + (x2_centroid2 - x2_obs5)**2)\n",
    "distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3620524583271009"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Observed values yi\n",
    "observed_y = np.array([22.1, 10.4, 9.3, 18.5, 12.9, 7.2, 11.8])\n",
    "\n",
    "# Predicted values ŷi from regression\n",
    "predicted_y = np.array([20.52, 12.34, 12.31, 17.60, 13.19, 12.48, 11.73])\n",
    "\n",
    "# Calculating the scaled deviance for gamma regression\n",
    "scaled_deviance = 2 * np.sum((observed_y - predicted_y) / predicted_y - np.log(observed_y / predicted_y))\n",
    "scaled_deviance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10371560108901645"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given data\n",
    "Y = np.array([0.8226, 0.7210, 0.9617, 0.2221, 0.1013])\n",
    "X = np.array([\n",
    "    [1, 0.5246, 0.4204],\n",
    "    [1, 0.9704, 0.6895],\n",
    "    [1, 0.8083, 0.2000],\n",
    "    [1, 0.1827, 0.3401],\n",
    "    [1, 0.3135, 0.5923]\n",
    "])\n",
    "beta = np.array([0.4171, 1.0068, -0.9256])\n",
    "\n",
    "# Leverage matrix H\n",
    "H = np.array([\n",
    "    [0.2069, 0.1337, 0.2249, 0.2423, 0.1922],\n",
    "    [0.1337, 0.8564, 0.0672, -0.2511, 0.1938],\n",
    "    [0.2249, 0.0672, 0.8449, 0.1139, -0.2509],\n",
    "    [0.2423, -0.2511, 0.1139, 0.5609, 0.3340],\n",
    "    [0.1922, 0.1938, -0.2509, 0.3340, 0.5309]\n",
    "])\n",
    "\n",
    "# Predicted values\n",
    "Y_hat = X @ beta\n",
    "\n",
    "# Residuals for each observation using LOOCV\n",
    "residuals = (Y - Y_hat) / (1 - np.diag(H))\n",
    "\n",
    "# Squared residuals and LOOCV MSE\n",
    "squared_residuals = residuals**2\n",
    "LOOCV_MSE = np.mean(squared_residuals)\n",
    "LOOCV_MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error of the one-dimensional approximation: 1.1630795100000002\n"
     ]
    }
   ],
   "source": [
    "# Standard deviation of each principal component\n",
    "standard_deviations = {\n",
    "    'PC1': 2.1993,\n",
    "    'PC2': 0.9823,\n",
    "    'PC3': 0.40453,\n",
    "    'PC4': 0.15143,\n",
    "    'PC5': 0.10126,\n",
    "    'PC6': 0.03587\n",
    "}\n",
    "\n",
    "# Total variance when data is standardized and has 6 dimensions\n",
    "total_variance = 6\n",
    "\n",
    "# Variance explained by the first principal component (PC1)\n",
    "variance_explained_by_PC1 = standard_deviations['PC1'] ** 2\n",
    "\n",
    "# Mean squared error of the one-dimensional approximation\n",
    "mse_one_dimensional = total_variance - variance_explained_by_PC1\n",
    "\n",
    "print(f\"Mean Squared Error of the one-dimensional approximation: {mse_one_dimensional}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
